# -*- coding: utf-8 -*-
"""Fake News Detection using BERT

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g90zAFgAv46ZUoVLjkiQGUnRxhikAvXk

<h1 align=center> Fake News Detection using BERT</h1>

## Install TensorFlow and TensorFlow Model Garden
"""

import tensorflow as tf
print(tf.version.VERSION)

!git clone --depth 1 -b v2.3.0 https://github.com/tensorflow/models.git

# installing requirements to use tensorflow/models repository
# Restart the runtime once installed (any errors to be ignored)
!pip install -Uqr models/official/requirements.txt

"""## After restarting the runtime, import libraries"""

# Importing libraries
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
import tensorflow as tf
import tensorflow_hub as hub
import sys
sys.path.append('models')
from official.nlp.data import classifier_data_lib
from official.nlp.bert import tokenization
from official.nlp import optimization
from google.colab import drive

print("TF Version: ", tf.__version__)
print("Eager mode: ", tf.executing_eagerly())
print("Hub version: ", hub.__version__)
print("GPU is", "available" if tf.config.experimental.list_physical_devices("GPU") else "NOT AVAILABLE")

"""## Reading in data"""

# Uploading input datasets to drive and mounting drive

drive.mount('/content/drive')

# Reading in input datasets
train_df_mod = pd.read_csv('drive/My Drive/train.csv')
test_df_mod = pd.read_csv('drive/My Drive/test.csv')

# Dropping duplicate records present in input train data
tr_df1 = train_df_mod[['title','author','text','label']].drop_duplicates()
tr_df1.shape

# Resetting index
tr_df1 = tr_df1.reset_index(drop=True)

# Creating a copy of input data for backup purposes
train_df_title = tr_df1.copy()

tr_df1.shape

# Empty title records earlier had a float datatype; converting all title records to string data type
train_df_title['normalized_cleaned_title'] = train_df_title['title'].apply(lambda x: str(x))

"""## Creating Train, Validation and Test splits"""

# Splitting data to create test set
X_trn, X_test, y_trn, y_test = train_test_split(train_df_title['normalized_cleaned_title'], train_df_title['label'], test_size=0.15, random_state=0, stratify=train_df_title['label'])

# Splitting data again to create train and validation data (needed as validation_split parameter doesn't work with tf.data.Dataset)
X_train, X_val, y_train, y_val = train_test_split(X_trn, y_trn, test_size=0.20, random_state=0, stratify=y_trn)

print("Train data shape: X = {}, y = {}".format(X_train.shape, y_train.shape))
print("Validation data shape: X = {}, y = {}".format(X_val.shape, y_val.shape))
print("Test data shape: X = {}, y = {}".format(X_test.shape, y_test.shape))

# Checking if there is a class imbalance
tr_df1['label'].value_counts()

"""## Create tf.data.Datasets for Training and Evaluation"""

with tf.device('/cpu:0'):
  train_data = tf.data.Dataset.from_tensor_slices((X_train.values, y_train.values))
  valid_data = tf.data.Dataset.from_tensor_slices((X_val.values, y_val.values))
  test_data = tf.data.Dataset.from_tensor_slices((X_test.values, y_test.values))

# Data check
for text,label in train_data.take(1):
  print(text)
  print(label)

"""## Fetching Pre-trained BERT Cased Model from TensorFlow Hub"""

"""
- Input data needs to be transformed to BERT input features:
input_word_ids, input_mask, segment_ids
- BERT model tokenizer is used to tokenize the input data
"""

label_list = [0,1] # Label categories (0: reliable news, 1: unreliable news)
max_seq_length = 128 # maximum length of (token) input sequences
train_batch_size = 32


# Fetching BERT layer and tokenizer:
bert_layer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/2', trainable=True)

vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()
do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()
tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)

"""## Tokenize and Preprocess Text for BERT"""

# Function to convert row to BERT input features and label

def to_feature(text, label, label_list=label_list, max_seq_length=max_seq_length, tokenizer=tokenizer):
  example = classifier_data_lib.InputExample(guid=None, text_a = text.numpy(), text_b = None, label=label.numpy())
  feature = classifier_data_lib.convert_single_example(0, example, label_list, max_seq_length, tokenizer)

  return (feature.input_ids, feature.input_mask, feature.segment_ids, feature.label_id)

# Wrapper function; needed to apply to_feature() to each element of the input data
def to_feature_map(text, label):
  input_ids, input_mask, segment_ids, label_id = tf.py_function(to_feature, inp=[text, label],
                                                                Tout=[tf.int32, tf.int32, tf.int32, tf.int32])
  input_ids.set_shape([max_seq_length])
  input_mask.set_shape([max_seq_length])
  segment_ids.set_shape([max_seq_length])
  label_id.set_shape([])

  x = {
      'input_word_ids': input_ids,
       'input_mask': input_mask,
       'input_type_ids': segment_ids

  }

  return (x, label_id)

"""## Creating TensorFlow Input Pipeline with `tf.data`"""

with tf.device('/cpu:0'):
  # train
  train_data = (train_data.map(to_feature_map,
                               num_parallel_calls=tf.data.experimental.AUTOTUNE)
  .shuffle(1000)
  .batch(32, drop_remainder=True)
  .prefetch(tf.data.experimental.AUTOTUNE)) # pre-fetching input data for pre-processing to optimize runtime and resource utilization

  # valid
  valid_data = (valid_data.map(to_feature_map,
                               num_parallel_calls=tf.data.experimental.AUTOTUNE)
  .batch(32) # , drop_remainder=True
  .prefetch(tf.data.experimental.AUTOTUNE))

  # test
  test_data = (test_data.map(to_feature_map,
                               num_parallel_calls=tf.data.experimental.AUTOTUNE)
  .batch(32) # , drop_remainder=True
  .prefetch(tf.data.experimental.AUTOTUNE))

# train data spec
train_data.element_spec

# valid data spec
valid_data.element_spec

# test data spec
test_data.element_spec

"""## Adding a Classification Head to the BERT Layer"""

# Building the model
def create_model():
  input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,
                                       name="input_word_ids")
  input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,
                                   name="input_mask")
  input_type_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,
                                    name="input_type_ids")
  
  pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, input_type_ids])

  # Using pooled output from BERT layer 
  drop = tf.keras.layers.Dropout(0.4)(pooled_output) # Dropout layer to avoid overfitting
  output = tf.keras.layers.Dense(1, activation='sigmoid', name='output')(drop) # Classification layer

  model = tf.keras.Model(
      inputs={
          'input_word_ids': input_word_ids,
          'input_mask': input_mask,
          'input_type_ids': input_type_ids
      },
      outputs=output
  )
  return model

"""## Fine-Tuning BERT for Text Classification"""

model = create_model()
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),
              loss=tf.keras.losses.BinaryCrossentropy(),
              metrics=[tf.keras.metrics.BinaryAccuracy()])
model.summary()

# Plotting the model
tf.keras.utils.plot_model(model=model, show_shapes=True, dpi=76)

# Model training
epochs = 2
history = model.fit(train_data,
                    validation_data=valid_data,
                    epochs=epochs,
                    verbose=1)

"""## Model Performance evaluation"""

import matplotlib.pyplot as plt

def plot_graphs(history, metric):
  plt.plot(history.history[metric])
  plt.plot(history.history['val_'+metric], '')
  plt.xlabel("Epochs")
  plt.ylabel(metric)
  plt.legend([metric, 'val_'+metric])
  plt.show()

plot_graphs(history, 'loss')

plot_graphs(history, 'binary_accuracy')

# Validation set performance
targ = []
for example in valid_data:
  for i in example[1]:
    targ.append(i.numpy())
print(len(targ))

val_pred = model.predict(valid_data)
thres = 0.5
valid_class = [1 if pred >= thres else 0 for pred in val_pred]

from sklearn.metrics import confusion_matrix, classification_report

print(classification_report(y_true = targ,y_pred = valid_class))
print(confusion_matrix(y_true = targ,y_pred = valid_class))

from sklearn.metrics import accuracy_score

accuracy_score(targ, valid_class)

# Test set performance
test_targ = []
for example in test_data:
  for i in example[1]:
    test_targ.append(i.numpy())
print(len(test_targ))

test_pred = model.predict(test_data)
test_class = [1 if pred >= thres else 0 for pred in test_pred]

print(classification_report(y_true = test_targ,y_pred = test_class))
print(confusion_matrix(y_true = test_targ,y_pred = test_class))
print(accuracy_score(test_targ, test_class))

# Prediction data
test_df_mod['normalized_cleaned_title'] = test_df_mod['title'].apply(lambda x: str(x))

pred_data = tf.data.Dataset.from_tensor_slices((test_df_mod['normalized_cleaned_title'].values, [0]*len(test_df_mod['normalized_cleaned_title'])))
pred_data = (pred_data.map(to_feature_map).batch(1))
preds = model.predict(pred_data)
predicted_class = [1 if pred >= thres else 0 for pred in preds]

# Saving model
export_dir = './saved_model'
model.save(export_dir)

# Loading saved model (for check purposes)
reloaded_model = tf.keras.models.load_model(export_dir)

# Checking if the model has been loaded correctly
reloaded_preds = reloaded_model.predict(pred_data)
reloaded_predicted_class = [1 if pred >= thres else 0 for pred in reloaded_preds]

assert predicted_class == reloaded_predicted_class

# Compressing the saved model
!zip -r /content/file.zip /content/saved_model

# Downloading compressed saved model
from google.colab import files
files.download("/content/file.zip")

"""## Preparing the Submission file"""

# Submission file
id = test_df_mod['id']

submit = pd.DataFrame({'id': id, 'label': predicted_class})

submit.shape

submit.head()

# Save the Submission file
submit.to_csv('./submit.csv', index=False)